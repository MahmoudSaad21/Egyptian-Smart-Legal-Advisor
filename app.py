# Imports arranged by category
import re
import time
import json
import os
from typing import List, Dict, Any, Literal

# Data processing
import torch
import numpy as np
import arabic_reshaper
import unicodedata
from bidi.algorithm import get_display
import pdfplumber
from tashaphyne.stemming import ArabicLightStemmer

# NLP and ML libraries
from transformers import pipeline

# Database
from neo4j import GraphDatabase

# Vector stores and embeddings
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# LLM chains and components
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableBranch, RunnableLambda
from langchain_groq import ChatGroq
from langchain_google_genai import ChatGoogleGenerativeAI

# Frontend
import streamlit as st
from streamlit_chat import message


# Apply RTL for Arabic UI
st.set_page_config(page_title="Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ", layout="wide")
st.markdown("""
    <style>
        body { direction: rtl; text-align: right; }
        .stTextInput > div > div > input { text-align: right; }
        .st-emotion-cache-16txtl3 { padding: 1rem 1rem 1rem 10rem; }
        .st-emotion-cache-13ejsyy { direction: rtl; }
    </style>
""", unsafe_allow_html=True)

# Arabic Text Processor for Normalization
class ArabicTextProcessor:
    def __init__(self):
        self.stopwords = self.load_arabic_stopwords()
        self.stemmer = ArabicLightStemmer()  # Light stemming for Arabic

    def load_arabic_stopwords(self):
        """Loads common Arabic stopwords."""
        return set(["ÙÙŠ", "Ù…Ù†", "Ø¹Ù„Ù‰", "Ùˆ", "Ø§Ù„", "Ø§Ù„Ù‰", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ù…Ø¹", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "Ø£Ùˆ", "Ø«Ù…", "Ù„ÙƒÙ†", "Ùˆ", "Ù", "Ø¨", "Ù„"])

    def normalize_arabic_text(self, text):
        """Normalize Arabic text by removing diacritics, normalizing letters, and stemming."""
        text = re.sub(r'[\u064B-\u065F\u0670]', '', text)  # Remove diacritics
        text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)  # Normalize alef
        text = text.replace('Ø©', 'Ù‡')  # Normalize teh marbuta
        text = text.replace('Ù‰', 'ÙŠ')  # Normalize yeh
        text = text.replace('Ù€', '')  # Remove kashida

        return text

    def extract_key_legal_terms(self, text):
        """Extract key legal terms from a scenario to improve retrieval"""
        legal_term_patterns = [
            r'(Ø¬Ø±ÙŠÙ…Ø©|ØªÙ‡Ù…Ø©|Ù‚Ø¶ÙŠØ©|Ø¯Ø¹ÙˆÙ‰|Ù…Ø­ÙƒÙ…Ø©|Ù‚Ø§Ù†ÙˆÙ†|Ø­ÙƒÙ…|Ø¹Ù‚ÙˆØ¨Ø©|Ø³Ø¬Ù†|ØºØ±Ø§Ù…Ø©|ØªØ¹ÙˆÙŠØ¶|Ù…Ø®Ø§Ù„ÙØ©|Ø¬Ù†Ø­Ø©|Ø¬Ù†Ø§ÙŠØ©)',
            r'(Ù…ØªÙ‡Ù…|Ø´Ø§Ù‡Ø¯|Ù…Ø­Ø§Ù…|Ù‚Ø§Ø¶ÙŠ|Ù…Ø¯Ø¹ÙŠ|Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡|Ù†ÙŠØ§Ø¨Ø©|ØªØ­Ù‚ÙŠÙ‚|Ù…Ø­Ø¶Ø±|Ø£Ø¯Ù„Ø©|Ø¥Ø«Ø¨Ø§Øª|Ø¨Ø±Ø§Ø¡Ø©|Ø¥Ø¯Ø§Ù†Ø©)',
            r'(Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª|Ø§Ø³ØªØ¦Ù†Ø§Ù|Ù†Ù‚Ø¶|Ø·Ø¹Ù†|Ø¯ÙØ§Ø¹|Ø§ØªÙ‡Ø§Ù…|Ù…Ø­Ø§ÙƒÙ…Ø©|Ø¬Ù„Ø³Ø©|Ø­Ø¨Ø³|ØªÙˆÙ‚ÙŠÙ|Ø§Ø­ØªØ¬Ø§Ø²|ØªÙØªÙŠØ´|Ø¶Ø¨Ø·)',
            r'(ØªÙ‚Ø§Ø¯Ù…|ØªÙ‚Ø§Ø¯Ù… Ù…ÙƒØ³Ø¨|ØªÙ‚Ø§Ø¯Ù… Ù…Ø³Ù‚Ø·|Ù…Ø¯Ø© Ø§Ù„ØªÙ‚Ø§Ø¯Ù…|Ù…Ø±ÙˆØ± Ø§Ù„Ø²Ù…Ù†|Ø§Ù†Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ø¯Ø¹ÙˆÙ‰|Ø³Ù‚ÙˆØ· Ø§Ù„Ø­Ù‚ Ø¨Ø§Ù„ØªÙ‚Ø§Ø¯Ù…)',
            r'(Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§Ù„Ø­Ù‚|Ø³ÙˆØ¡ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§Ù„Ø­Ù‚|ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ù‚|Ø­Ù‚ Ù…Ø´Ø±ÙˆØ¹|Ø­Ù‚ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ¹|Ø¥Ø³Ø§Ø¡Ø© Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§Ù„Ø­Ù‚)',
            r'(Ø¬Ø±ÙŠÙ…Ø©|Ø¯Ø¹ÙˆÙ‰|Ù…Ø­ÙƒÙ…Ø©|Ø­ÙƒÙ…|Ù‚Ø§Ù†ÙˆÙ†|Ø§Ù„ØªØ²Ø§Ù…|Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©|Ø¹Ù‚Ø¯|Ø§Ù„ØªØ²Ø§Ù… Ù‚Ø§Ù†ÙˆÙ†ÙŠ|Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª|Ø­Ù‚ÙˆÙ‚|Ù…Ø®Ø§Ù„ÙØ©)',
            r'(Ù…Ø¯Ø¹ÙŠ|Ù…Ø¯Ø¹Ù‰ Ø¹Ù„ÙŠÙ‡|Ù…Ø­Ø§Ù…|Ù‚Ø§Ø¶ÙŠ|ØªØ­Ù‚ÙŠÙ‚|Ø¥Ø«Ø¨Ø§Øª|Ø¨Ø±Ø§Ø¡Ø©|Ø¥Ø¯Ø§Ù†Ø©|Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©|Ø¹Ù‚ÙˆØ¨Ø©|Ø¯Ø¹ÙˆÙ‰ Ù…Ø¯Ù†ÙŠØ©)',
            r'(Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ù…Ø±Ø£Ø©|Ø§Ù„Ù…Ø³Ø§ÙˆØ§Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¬Ù†Ø³ÙŠÙ†|ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ù…Ø±Ø£Ø©|Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø±Ø£Ø©|Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø³ÙŠØ§Ø³ÙŠ Ù„Ù„Ù…Ø±Ø£Ø©)',
            r'(Ø¹Ø¯Ù… Ø§Ù„ØªÙ…ÙŠÙŠØ²|Ø§Ù„Ø­Ù‚ÙˆÙ‚ Ø§Ù„Ø¯Ø³ØªÙˆØ±ÙŠØ©|Ø§Ù„Ù…Ø´Ø§Ø±ÙƒØ© Ø§Ù„Ø³ÙŠØ§Ø³ÙŠØ©|Ø§Ù„Ø¹Ù…Ù„|Ø§Ù„Ø£Ø¬ÙˆØ±|Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©)',
            r'(Ø§Ù„Ø¯Ø³ØªÙˆØ± Ø§Ù„Ù…ØµØ±ÙŠ|Ø§Ù„Ù…Ø¬Ù„Ø³ Ø§Ù„Ù‚ÙˆÙ…ÙŠ Ù„Ù„Ù…Ø±Ø£Ø©|Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ|Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠ|Ø§Ù„Ø£Ø­ÙˆØ§Ù„ Ø§Ù„Ø´Ø®ØµÙŠØ©)',
            r'(Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§Ù„Ø­Ù‚|Ø³ÙˆØ¡ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø§Ù„Ø­Ù‚|ØªØ¬Ø§ÙˆØ² Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ù‚|Ø­Ù‚ Ù…Ø´Ø±ÙˆØ¹|Ø­Ù‚ ØºÙŠØ± Ù…Ø´Ø±ÙˆØ¹)',
            r'(Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ|Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø§Ù„ØªÙ‚ØµÙŠØ±ÙŠØ©|Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ø§Ù„Ø¹Ù‚Ø¯ÙŠØ©|Ø§Ù„ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù…Ø¯Ù†ÙŠ)',
            r'(Ø§Ù„Ø¹Ù‚Ø¯|Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…|Ø§Ù„Ø¥Ø±Ø§Ø¯Ø© Ø§Ù„Ù…Ù†ÙØ±Ø¯Ø©|Ø§Ù„Ø¥Ø«Ø±Ø§Ø¡ Ø¨Ù„Ø§ Ø³Ø¨Ø¨|Ø§Ù„ÙØ¹Ù„ Ø§Ù„Ø¶Ø§Ø±|Ø§Ù„Ø¶Ø±Ø±)'
        ]

        key_terms = []
        for pattern in legal_term_patterns:
            matches = re.findall(pattern, text)
            key_terms.extend([self.stemmer.light_stem(word) for word in matches])

        return list(set(key_terms))  # Return unique terms

# Function to extract text from Arabic PDF using pdfplumber
def extract_text_from_pdf(pdf_path):
    """Extracts Arabic text from a PDF file and fixes reversed text issues"""
    extracted_text = ""

    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text()
            if text:
                reshaped_text = arabic_reshaper.reshape(text)  # Fix Arabic shaping
                fixed_text = get_display(reshaped_text)  # Correct text order
                extracted_text += fixed_text + "\n"

    if not extracted_text.strip():
        st.warning(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù…Ø³ØªØ®Ø±Ø¬ Ù…Ù† {pdf_path}! ØªØ­Ù‚Ù‚ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ù PDF Ù…Ù…Ø³ÙˆØ­Ù‹Ø§ Ø¶ÙˆØ¦ÙŠÙ‹Ø§ Ø£Ùˆ ÙŠØ³ØªØ®Ø¯Ù… Ø®Ø·ÙˆØ·Ù‹Ø§ Ù…Ø¶Ù…Ù†Ø©.")

    normalized_text = unicodedata.normalize("NFKC", extracted_text)
    normalized_text = re.sub(r'\bÙ„Ø§(?=[Ø§-ÙŠ])', 'Ø§Ù„', normalized_text)  # Beginning of word
    normalized_text = re.sub(r'(?<=[Ø§-ÙŠ])Ù„Ø§\b', 'Ø§Ù„', normalized_text)  # End of word
    normalized_text = re.sub(r"\)([^\)]+)\(", r"(\1)", normalized_text)
    return normalized_text

# Function to split legal text based on article numbers and law source
def split_legal_text(text, source_name):
    """Splits legal text into structured chunks using Arabic article numbers."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=[".", "ØŸ", "\n\n"]
    )

    pages = splitter.split_text(text)
    # Rebuild chunks, ensuring each article stays with its content
    chunks = []
    for i, page in enumerate(pages, start=1):  # Start chunk numbering from 1
        chunks.append({
            "content": page.strip(),
            "source": source_name,
            "article_id": str(i)  # Using chunk number as article_id
        })

    return chunks

# Function to load and process multiple Arabic PDFs
def load_and_process_pdfs(legal_files, progress_bar=None):
    """Loads and processes multiple Arabic PDFs for retrieval"""
    all_chunks = []
    total_files = len(legal_files)

    for idx, (pdf_path, source_name) in enumerate(legal_files):
        if progress_bar:
            progress_bar.progress((idx) / total_files, text=f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {source_name}...")

        # Extract text using pdfplumber
        extracted_text = extract_text_from_pdf(pdf_path)

        if not extracted_text:
            if progress_bar:
                progress_bar.warning(f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† {pdf_path}")
            continue

        # Split text into structured chunks with source information
        chunks = split_legal_text(extracted_text, source_name)
        all_chunks.extend(chunks)

        if progress_bar:
            progress_bar.progress((idx + 1) / total_files, text=f"ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {source_name} Ø¨Ù†Ø¬Ø§Ø­!")

    if not all_chunks:
        if progress_bar:
            progress_bar.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙŠ Ø£Ø¬Ø²Ø§Ø¡ ØµØ§Ù„Ø­Ø© Ù…Ù† Ø£ÙŠ Ù…Ø³ØªÙ†Ø¯!")
        return []

    return all_chunks

# Neo4j Graph Database Handler
class LegalGraphDB:
    def __init__(self, uri, username, password):
        self.driver = GraphDatabase.driver(uri, auth=(username, password))
        self.setup_schema()

    def close(self):
        self.driver.close()

    def setup_schema(self):
        """Setup Neo4j schema with constraints and indexes"""
        with self.driver.session() as session:
            # Create constraints for uniqueness
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (l:LegalArticle) REQUIRE l.id IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (s:Source) REQUIRE s.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (t:Term) REQUIRE t.name IS UNIQUE")

            # Create indexes for faster retrieval
            session.run("CREATE INDEX IF NOT EXISTS FOR (l:LegalArticle) ON (l.article_id)")
            session.run("CREATE INDEX IF NOT EXISTS FOR (t:Term) ON (t.name)")

    def add_legal_chunks(self, chunks, progress_bar=None):
        """Add legal chunks to Neo4j with metadata and relationships"""
        total_chunks = len(chunks)

        with self.driver.session() as session:
            for i, chunk in enumerate(chunks):
                if progress_bar and i % 10 == 0:  # Update progress every 10 chunks
                    progress_bar.progress((i) / total_chunks, text=f"Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Neo4j... {i}/{total_chunks}")

                # Generate a unique ID for the chunk
                chunk_id = f"{chunk['source']}-{chunk['article_id']}-{hash(chunk['content'])}"

                # Create the legal article node with all metadata
                session.run("""
                    MERGE (s:Source {name: $source})
                    MERGE (l:LegalArticle {
                        id: $id,
                        content: $content,
                        article_id: $article_id
                    })
                    MERGE (l)-[:FROM_SOURCE]->(s)
                """, {
                    "id": chunk_id,
                    "content": chunk["content"],
                    "article_id": chunk["article_id"],
                    "source": chunk["source"]
                })

                # Extract key terms and create relationships
                processor = ArabicTextProcessor()
                key_terms = processor.extract_key_legal_terms(chunk["content"])

                for term in key_terms:
                    session.run("""
                        MATCH (l:LegalArticle {id: $chunk_id})
                        MERGE (t:Term {name: $term})
                        CREATE (l)-[:MENTIONS]->(t)
                    """, {
                        "chunk_id": chunk_id,
                        "term": term
                    })

            if progress_bar:
                progress_bar.progress(1.0, text="ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Neo4j Ø¨Ù†Ø¬Ø§Ø­!")

    def search_related_articles(self, terms, limit=5):
        """Search for related articles based on legal terms"""
        with self.driver.session() as session:
            result = session.run("""
                UNWIND $terms AS term
                MATCH (t:Term {name: term})<-[:MENTIONS]-(l:LegalArticle)-[:FROM_SOURCE]->(s:Source)
                RETURN l.id AS id, l.content AS content, l.article_id AS article_id,
                       s.name AS source, count(t) AS relevance
                ORDER BY relevance DESC
                LIMIT $limit
            """, {"terms": terms, "limit": limit})

            return [record.data() for record in result]

    def search_contextual_articles(self, article_id, source, limit=3):
        """Find contextually related articles based on legal hierarchy"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (l:LegalArticle)-[:FROM_SOURCE]->(s:Source {name: $source})
                WHERE abs(toInteger(l.article_id) - toInteger($article_id)) <= 5
                AND l.article_id <> $article_id
                RETURN l.id AS id, l.content AS content, l.article_id AS article_id,
                      s.name AS source
                ORDER BY abs(toInteger(l.article_id) - toInteger($article_id))
                LIMIT $limit
            """, {"article_id": article_id, "source": source, "limit": limit})

            return [record.data() for record in result]

    def check_database_populated(self):
        """Check if the database has any data"""
        with self.driver.session() as session:
            result = session.run("MATCH (l:LegalArticle) RETURN count(l) as count")
            record = result.single()
            if record and record["count"] > 0:
                return True
            return False

# Function to create vector embeddings using FAISS with an Arabic model
def create_faiss_embeddings(chunks, progress_bar=None):
    """Creates FAISS vector embeddings for the text using an Arabic-optimized model"""
    if not chunks:
        if progress_bar:
            progress_bar.warning("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£Ø¬Ø²Ø§Ø¡ Ù†ØµÙŠØ© Ù„Ù„ØªØ¶Ù…ÙŠÙ†. ØªØ®Ø·ÙŠ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¶Ù…ÙŠÙ†!")
        return None

    if progress_bar:
        progress_bar.progress(0.3, text="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")

    # Extract just the content field for embedding
    texts = [chunk["content"] for chunk in chunks]
    metadatas = [{"source": chunk["source"], "article_id": chunk["article_id"]} for chunk in chunks]

    embedding_model = HuggingFaceEmbeddings(
        model_name="silma-ai/silma-embeddding-sts-v0.1",  # Arabic-optimized embedding model
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    if progress_bar:
        progress_bar.progress(0.6, text="Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª...")

    vectordb = FAISS.from_texts(
        texts=texts,
        embedding=embedding_model,
        metadatas=metadatas
    )

    # Save the FAISS index for future use
    vectordb.save_local("./arabic_law_faiss")

    if progress_bar:
        progress_bar.progress(1.0, text="ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS Ø¨Ù†Ø¬Ø§Ø­!")

    return vectordb

# Setup query classifier using a more robust approach
def setup_query_classifier():
    """Setup a classifier for legal query types"""
    # Using Arabic-compatible multilingual model
    classifier = pipeline(
        "zero-shot-classification",
        model="joeddav/xlm-roberta-large-xnli",
    )

    def classify_query(query):
        # Define classes in Arabic
        classes = [
            "Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù‚Ø§Ù†ÙˆÙ† Ù…Ø­Ø¯Ø¯",     # Question about specific law
            "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø­Ø§Ù„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©", # Legal scenario inquiry
            "Ø·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©",     # Request for legal advice
            "Ø³Ø¤Ø§Ù„ Ø¹Ø§Ù…",                # General question
            "Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø§Ù…Ø©"               # General chat
        ]

        result = classifier(
            query,
            candidate_labels=classes,
            hypothesis_template="Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØªØ¹Ù„Ù‚ Ø¨Ù€ {}."
        )

        # Get the top class and confidence
        top_class = result["labels"][0]
        confidence = result["scores"][0]

        # Map to simplified categories
        if top_class in ["Ø³Ø¤Ø§Ù„ Ø¹Ù† Ù‚Ø§Ù†ÙˆÙ† Ù…Ø­Ø¯Ø¯", "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ù† Ø­Ø§Ù„Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©", "Ø·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"]:
            query_type = "legal_query"
        else:
            query_type = "general_query"

        return {
            "detailed_type": top_class,
            "query_type": query_type,
            "confidence": confidence
        }

    return classify_query

# Enhanced retrieval function integrating FAISS and Neo4j
def hybrid_retrieval(query, faiss_db, graph_db, top_k=10):
    """Perform hybrid retrieval combining vector search and graph traversal"""
    processor = ArabicTextProcessor()

    # Step 1: Extract key legal terms for Neo4j search
    legal_terms = processor.extract_key_legal_terms(query)

    # Step 2: Perform FAISS vector similarity search
    normalized_query = processor.normalize_arabic_text(query)
    vector_results = faiss_db.similarity_search_with_score(query, k=top_k)

    # Step 3: Perform Neo4j graph-based search
    graph_results = []
    if legal_terms:
        graph_results = graph_db.search_related_articles(legal_terms, limit=top_k)

    # Step 4: Combine and rank results
    combined_results = []

    # Process vector results
    for doc, score in vector_results:
        retrieved_doc = {
            "content": doc.page_content,
            "source": doc.metadata.get("source", "Unknown"),
            "article_id": doc.metadata.get("article_id", "Unknown"),
            "vector_score": score,
            "graph_score": 0,
            "total_score": 1 / (1 + score)  # Convert distance to similarity score
        }
        combined_results.append(retrieved_doc)

    # Process graph results and merge with vector results if they exist
    for result in graph_results:
        # Check if this result already exists in combined_results
        existing = next((item for item in combined_results if
                         item["content"] == result["content"] and
                         item["source"] == result["source"]), None)

        if existing:
            # Update the existing entry with graph score
            existing["graph_score"] = result["relevance"]
            existing["total_score"] = existing["total_score"] + (result["relevance"] * 0.2)  # Weight graph results
        else:
            # Add new entry
            retrieved_doc = {
                "content": result["content"],
                "source": result["source"],
                "article_id": result["article_id"],
                "vector_score": 0,
                "graph_score": result["relevance"],
                "total_score": result["relevance"] * 0.2  # Weight graph results
            }
            combined_results.append(retrieved_doc)

    # Step 5: Sort by total score and get top results
    combined_results.sort(key=lambda x: x["total_score"], reverse=True)

    # Step 6: Context expansion - for top results, find contextually related articles
    top_results = combined_results[:min(5, len(combined_results))]
    expanded_results = list(top_results)

    for result in top_results:
        if result["article_id"] != "ØºÙŠØ± Ù…Ø­Ø¯Ø¯":
            related = graph_db.search_contextual_articles(
                result["article_id"],
                result["source"],
                limit=2
            )

            for rel in related:
                # Check if already in results
                if not any(r["content"] == rel["content"] for r in expanded_results):
                    expanded_results.append({
                        "content": rel["content"],
                        "source": rel["source"],
                        "article_id": rel["article_id"],
                        "vector_score": 0,
                        "graph_score": 0.3,
                        "total_score": 0.3,
                        "context_relation": f"Related to Article {result['article_id']}"
                    })

    # Get final top results (limit to reasonable number)
    final_results = expanded_results[:min(10, len(expanded_results))]

    # Format for LLM
    formatted_results = []
    for i, res in enumerate(final_results):
        formatted_results.append(
            f"ã€{i+1}ã€‘ Ø§Ù„Ù…ØµØ¯Ø±: {res['source']} | Ø§Ù„Ù…Ø§Ø¯Ø©: {res['article_id']}\n{res['content']}\n"
        )

    return "\n\n".join(formatted_results)

# Function to build legal advisor chain with context handling
def build_legal_advisor_chain(faiss_db, graph_db):
    """Builds an enhanced legal advisor chain with context handling"""
    # Set Google API key
    os.environ["GOOGLE_API_KEY"] = "AIzaSyAmzcdK1AXIndg6LTR0QXhV4mNy4hMbkqY"  # Replace with your actual API key

    # Create LLM
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0.2
    )

    # Create enhanced prompt template with history and context
    prompt_template = """
    Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØµØ±ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠØŒ Ù…Ù‡Ù…ØªÙƒ ØªÙ‚Ø¯ÙŠÙ… Ù…Ø´ÙˆØ±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.

    # Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ:
    {query}

    # Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©:
    {context}

    # Ø§Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù„Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª (Ø¥Ù† ÙˆØ¬Ø¯):
    {history}

    # Ù…Ù‡Ù…ØªÙƒ:
    1. Ø­Ù„Ù„ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø± Ø¨Ø¯Ù‚Ø© ÙˆØ­Ø¯Ø¯ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠÙ‡.
    2. Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©.
    3. Ø§Ø´Ø±Ø­ ÙƒÙŠÙ ØªÙ†Ø·Ø¨Ù‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙˆØ§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø©.
    4. Ù‚Ø¯Ù… Ø§Ù„Ù…Ø´ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙÙ‚Ø·ØŒ ÙˆÙ„ÙŠØ³ Ø±Ø£ÙŠÙƒ Ø§Ù„Ø´Ø®ØµÙŠ.
    5. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„ØªÙŠ Ø§Ø³ØªÙ†Ø¯Øª Ø¥Ù„ÙŠÙ‡Ø§ Ø¨ÙˆØ¶ÙˆØ­.
    6. ØªØ¬Ù†Ø¨ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø¯Ø¹Ù…Ø© Ø¨Ù†ØµÙˆØµ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.

    # Ø§Ù„Ù…Ø´ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©:
    """

    # Function to retrieve documents
    def retrieve_docs(query):
        return hybrid_retrieval(query, faiss_db, graph_db)

    # Query history handler
    def process_with_history(query):
        # Add to history (limiting to last 3 queries for context)
        if 'query_history' not in st.session_state:
            st.session_state.query_history = []

        st.session_state.query_history.append(query)
        if len(st.session_state.query_history) > 3:
            st.session_state.query_history.pop(0)

        # Format history
        history_text = ""
        if len(st.session_state.query_history) > 1:
            history_text = "Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n" + "\n".join([
                f"- {q}" for q in st.session_state.query_history[:-1]
            ])

        # Get context from retrieval
        context = retrieve_docs(query)

        return {
            "query": query,
            "context": context,
            "history": history_text
        }

    # Create the chain
    legal_chain = (
        RunnableLambda(process_with_history)
        | PromptTemplate.from_template(prompt_template)
        | llm
        | StrOutputParser()
    )

    return legal_chain

# Function to build general conversation chain
def build_general_chain():
    """Builds a chain for general conversation"""
    # Set Google API key
    os.environ["GOOGLE_API_KEY"] = "Your API key"  # Replace with your actual API key

    # Create LLM
    llm = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        google_api_key=os.environ["GOOGLE_API_KEY"],
        temperature=0.7
    )

    prompt = PromptTemplate.from_template("""
    Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙˆØ¯ÙˆØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¯ Ø¹Ù„Ù‰ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ ÙˆÙ„Ø·ÙŠÙ:

    Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {query}
    """)

    return prompt | llm | StrOutputParser()

# Function to build complete system with routing
def initialize_system():
    """Initialize the legal advisory system once and store in session state"""
    if 'system_initialized' in st.session_state and st.session_state.system_initialized:
        return True

    # Define legal files to process
    legal_files = [
        ("/content/Ø§Ù„Ø¯Ø³ØªÙˆØ± Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„Ù…Ø¹Ø¯Ù„ 2019.pdf", "Ø§Ù„Ø¯Ø³ØªÙˆØ±"),
        ("/content/Ø§Ù„Ù…Ø¯Ù†ÙŠ.pdf", "Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ"),
        ("/content/Ù‚Ø§Ù†ÙˆÙ†_Ø§Ù„Ø§Ø¬Ø±Ø§Ø¡Ø§Øª_Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©.pdf", "Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©")
    ]

    # Neo4j configuration
    neo4j_config = {
        "uri": "Your URI",
        "username": "neo4j",
        "password": "Your_password"
    }

    # Create progress container
    progress_container = st.empty()

    with progress_container.container():
        st.markdown("## ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ")
        progress_bar = st.progress(0.0, text="Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")

        # Step 1: Connect to Neo4j and check if database already populated
        progress_bar.progress(0.1, text="Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Neo4j...")
        graph_db = LegalGraphDB(
            neo4j_config["uri"],
            neo4j_config["username"],
            neo4j_config["password"]
        )

        # Check if database already populated
        db_populated = graph_db.check_database_populated()

        # Step 2: Load and process data if needed
        if db_populated and os.path.exists("./arabic_law_faiss"):
            progress_bar.progress(0.5, text="Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...")
            st.success("ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø¬Ø§Ø±ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§...")

            # Load existing FAISS database
            progress_bar.progress(0.8, text="Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
            embedding_model = HuggingFaceEmbeddings(
                model_name="silma-ai/silma-embeddding-sts-v0.1",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            faiss_db = FAISS.load_local("./arabic_law_faiss", embedding_model, allow_dangerous_deserialization=True)


        else:
            st.info("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø©. Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø©...")

            # Process PDFs
            progress_bar.progress(0.2, text="Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª PDF...")
            all_chunks = load_and_process_pdfs(legal_files, progress_bar)

            if not all_chunks:
                st.error("âŒ ÙØ´Ù„ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©!")
                return False

            # Create FAISS vector database
            progress_bar.progress(0.5, text="Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª FAISS...")
            faiss_db = create_faiss_embeddings(all_chunks, progress_bar)

            # Add chunks to Neo4j
            progress_bar.progress(0.7, text="Ø¬Ø§Ø±ÙŠ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Neo4j...")
            graph_db.add_legal_chunks(all_chunks, progress_bar)

        # Step 3: Set up query classifier
        progress_bar.progress(0.9, text="Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø¯Ø§Ø¯ Ù…ØµÙ†Ù Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª...")
        query_classifier = setup_query_classifier()

        # Step 4: Build advisor chains
        progress_bar.progress(0.95, text="Ø¬Ø§Ø±ÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©...")
        legal_chain = build_legal_advisor_chain(faiss_db, graph_db)
        general_chain = build_general_chain()

        # Store components in session state
        st.session_state.faiss_db = faiss_db
        st.session_state.graph_db = graph_db
        st.session_state.query_classifier = query_classifier
        st.session_state.legal_chain = legal_chain
        st.session_state.general_chain = general_chain
        st.session_state.system_initialized = True

        progress_bar.progress(1.0, text="ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ù†Ø¬Ø§Ø­!")
        time.sleep(1)  # Short pause to show completion

    progress_container.empty()  # Clear the progress interface after completion
    return True

# Function to route queries to the right chain
def route_query(query_text):
    """Routes a query to either legal or general chain based on classification"""
    # Classify query
    classification = st.session_state.query_classifier(query_text)

    # Route to appropriate chain
    if classification["query_type"] == "legal_query":
        return {
            "chain": st.session_state.legal_chain,
            "query_type": "legal_query",
            "detailed_type": classification["detailed_type"]
        }
    else:
        return {
            "chain": st.session_state.general_chain,
            "query_type": "general_query",
            "detailed_type": classification["detailed_type"]
        }


# Main UI function
def main():
    st.title("Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ ğŸ§‘â€âš–ï¸")
    st.markdown("""
    Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠØŒ Ù…Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„Ø´Ø®ØµÙŠ Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ø³ØªÙØ³Ø§Ø±Ø§ØªÙƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
    ÙˆÙÙ‚Ù‹Ø§ Ù„Ù„Ù‚ÙˆØ§Ù†ÙŠÙ† Ø§Ù„Ù…ØµØ±ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„ØªÙƒ Ø­ÙˆÙ„:

    - Ø§Ù„Ø¯Ø³ØªÙˆØ± Ø§Ù„Ù…ØµØ±ÙŠ
    - Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ
    - Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©
    """)

    # Initialize the system once
    system_ready = initialize_system()

    if not system_ready:
        st.error("âŒ ÙØ´Ù„ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
        return

    # Initialize chat history
    if 'messages' not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨Ùƒ! Ø£Ù†Ø§ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ… ÙÙŠ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø© Ø¨Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠØŸ"}
        ]

    # Display chat messages - Fixed method for displaying chat messages
    for i, msg in enumerate(st.session_state.messages):
        if msg["role"] == "user":
            st.chat_message("user").write(msg["content"])
        else:
            st.chat_message("assistant").write(msg["content"])

    # Chat input
    if prompt := st.chat_input("Ø§ÙƒØªØ¨ Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‡Ù†Ø§..."):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        message(prompt, is_user=True, key=str(hash(prompt)))

        # Show thinking indicator
        with st.status("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙÙƒÙŠØ±...", expanded=True) as status:
            # Route the query
            router_result = route_query(prompt)

            # Display query classification
            query_type_arabic = "Ø§Ø³ØªÙØ³Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ" if router_result["query_type"] == "legal_query" else "Ø§Ø³ØªÙØ³Ø§Ø± Ø¹Ø§Ù…"
            st.write(f"Ù†ÙˆØ¹ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±: {query_type_arabic} ({router_result['detailed_type']})")

            # Process with appropriate chain
            try:
                response = router_result["chain"].invoke(prompt)
                status.update(label="ØªÙ…!", state="complete", expanded=False)
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ: {str(e)}")
                response = "Ø¹Ø°Ø±Ù‹Ø§ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªÙØ³Ø§Ø±Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ø³Ø¤Ø§Ù„Ùƒ."

        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": response})
        message(response, is_user=False)

# Add feedback mechanism
def add_feedback_section():
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©")
    col1, col2 = st.sidebar.columns(2)

    with col1:
        if st.button("ğŸ‘ Ù…ÙÙŠØ¯"):
            st.success("Ø´ÙƒØ±Ù‹Ø§ Ø¹Ù„Ù‰ ØªÙ‚ÙŠÙŠÙ…Ùƒ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ!")

    with col2:
        if st.button("ğŸ‘ ØºÙŠØ± Ù…ÙÙŠØ¯"):
            feedback = st.text_area("Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù†Ø§ ØªØ­Ø³ÙŠÙ†Ù‡ØŸ")
            if st.button("Ø¥Ø±Ø³Ø§Ù„"):
                st.success("Ø´ÙƒØ±Ù‹Ø§ Ø¹Ù„Ù‰ Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙƒ!")

# Add sidebar information
def add_sidebar():
    st.sidebar.title("Ø­ÙˆÙ„ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ")
    st.sidebar.markdown("""
    **Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ø°ÙƒÙŠ** Ù‡Ùˆ Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠØŒ ÙŠØ³Ø§Ø¹Ø¯Ùƒ ÙÙŠ:

    - ÙÙ‡Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
    - Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙØ³Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
    - ØªÙˆÙÙŠØ± Ù…Ø±Ø§Ø¬Ø¹ Ù„Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©

    ğŸ’¡ **Ù…Ù„Ø§Ø­Ø¸Ø© Ù‡Ø§Ù…Ø©**: Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…ØµÙ…Ù… Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙ‚Ø· ÙˆÙ„Ø§ ÙŠØºÙ†ÙŠ Ø¹Ù† Ø§Ø³ØªØ´Ø§Ø±Ø© Ù…Ø­Ø§Ù…Ù Ù…Ø¤Ù‡Ù„.
    """)

    # Add databases info
    st.sidebar.markdown("### Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    st.sidebar.markdown("""
    - Ø§Ù„Ø¯Ø³ØªÙˆØ± Ø§Ù„Ù…ØµØ±ÙŠ Ø§Ù„Ù…Ø¹Ø¯Ù„ 2019
    - Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…Ø¯Ù†ÙŠ Ø§Ù„Ù…ØµØ±ÙŠ
    - Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø¬Ù†Ø§Ø¦ÙŠØ©
    """)

    # Add feedback section
    add_feedback_section()

# Run the application
if __name__ == "__main__":
    add_sidebar()
    main()
